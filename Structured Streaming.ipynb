{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import TimestampType"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00509b20-b68d-4f93-b22c-7de491270d6a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["inputPath = \"/databricks-datasets/structured-streaming/events/\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f082d07-6496-43aa-a48c-cce6d977a581"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\n#While creating a PySpark DataFrame we can specify the structure \n#          using StructType and StructField classes. As specified in the introduction, StructType is a collection of StructFieldâ€™s \n#          which is used to define the column name, data type, and a flag for nullable or not.\njsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"975e2668-149f-47ca-8924-8b1a0b3ac1e0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(jsonSchema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .json(inputPath)\n)\n\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  streamingInputDF\n    .groupBy(\n      streamingInputDF.action, \n      window(streamingInputDF.time, \"1 hour\"))\n    .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5dbda5b2-47d0-4e44-b52b-7b3ba1319fb2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[25]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[25]: True"]}}],"execution_count":0},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table \n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4fac4b2-feb7-42d5-81ed-cab445534821"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["query.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54052725-2bea-4f0a-bc21-8d8b1579cf4c"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Structured Streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3162683516273479}},"nbformat":4,"nbformat_minor":0}
